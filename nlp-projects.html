<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>NLP Projects | Bota Duisenbay</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header>
    <h1>NLP Projects</h1>
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <section>
      <h2>HW1: Named Entity Recognition (NER)</h2>
      <p>
        Built and evaluated multiple BiLSTM-based models using GloVe embeddings for NER with IOB tagging.  
        Preprocessing significantly improved performance by reducing unknown tokens.  
        Best F1 score: <strong>0.5387</strong> using 2-layer BiLSTM with 300-d embeddings and dropout.
      </p>
      <a href="https://github.com/botastark/NLP/tree/main/duisenbay_1849680_hw1" target="_blank">View on GitHub</a>
    </section>

    <section>
      <h2>HW2: Semantic Role Labeling (SRL)</h2>
      <p>
        Implemented SRL using both GloVe and BERT embeddings.  
        Created a token-wise dataset per predicate with role classification.  
        Findings showed predicate indication method heavily influences performance.
      </p>
      <a href="https://github.com/botastark/NLP/tree/main/duisenbay_1849680_hw2" target="_blank">View on GitHub</a>
    </section>

    <section>
      <h2>HW3: Coreference Resolution</h2>
      <p>
        Modeled pronoun resolution using ALBERT, BERT, and RoBERTa.  
        Best performance achieved by RoBERTa.  
        Tagged entity/pronoun positions for effective encoding.  
        Used binary classification on ProBERT features.
      </p>
      <a href="https://github.com/botastark/NLP/tree/main/duisenbay_1849680_hw3" target="_blank">View on GitHub</a>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 Bota Duisenbay. All rights reserved.</p>
  </footer>
</body>
</html>
